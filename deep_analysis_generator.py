# ==============================================================================
# deep_analysis_generator.py - Generador de an√°lisis profundo con Ollama + Qdrant
# ==============================================================================

import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics
import time
from pathlib import Path

from config import Config
from analyzer_ollama import OllamaAnalyzer
from vector_store_qdrant import QdrantVectorStore
from database_manager import DatabaseManager


def integrate_deep_analysis_to_moderador(
    moderador_instance,
    messages,
    vector_store,
    analyzer,
    output_dir,
    log_file_name
):
    """Integra el generador de an√°lisis profundo al ModeradorSemantico"""

    deep_analyzer = DeepAnalysisGenerator(
        config=moderador_instance.config,
        db_manager=moderador_instance.db,
        vector_store=vector_store,
        ollama_analyzer=analyzer
    )

    # Agregar m√©todo al ModeradorSemantico
    def generate_deep_analysis(self, save_report: bool = True):
        """Genera an√°lisis profundo del chat procesado"""
        report = deep_analyzer.generate_comprehensive_analysis()

        if save_report and report:
            filepath = deep_analyzer.save_analysis_report(report)
            print(f"üìä An√°lisis profundo completado y guardado en: {filepath}")

        return report

    def get_live_dashboard(self):
        """Obtiene datos para dashboard en tiempo real"""
        return deep_analyzer.generate_live_dashboard_data()

    # Inyectar m√©todos
    import types
    moderador_instance.generate_deep_analysis = types.MethodType(generate_deep_analysis, moderador_instance)
    moderador_instance.get_live_dashboard = types.MethodType(get_live_dashboard, moderador_instance)

    return moderador_instance


class DeepAnalysisGenerator:
    """Genera an√°lisis profundos combinando datos de Qdrant y an√°lisis con Ollama"""

    def __init__(self, config: Config, db_manager: DatabaseManager,
                 vector_store: QdrantVectorStore, ollama_analyzer: OllamaAnalyzer):
        self.config = config
        self.db = db_manager
        self.vector_store = vector_store
        self.ollama = ollama_analyzer

    def generate_comprehensive_analysis(self, file_path: str = None) -> str:
        """Genera un an√°lisis completo y detallado del chat"""

        print("üß† Iniciando an√°lisis profundo con IA...")
        start_time = time.time()

        # 1. Obtener datos base de Qdrant
        collection_stats = self.vector_store.get_collection_stats()
        if "error" in collection_stats:
            return f"‚ùå Error obteniendo datos: {collection_stats['error']}"

        # 2. Obtener datos detallados
        detailed_data = self._get_detailed_analysis_data()

        # 3. Generar an√°lisis con Ollama
        analysis_sections = self._generate_analysis_sections(collection_stats, detailed_data)

        # 4. Compilar reporte final
        final_report = self._compile_final_report(analysis_sections, start_time)

        return final_report

    def _get_detailed_analysis_data(self) -> Dict:
        """Extrae datos detallados de Qdrant para an√°lisis profundo"""
        try:
            # Obtener todos los mensajes con payload completo
            all_points, _ = self.vector_store.client.scroll(
                collection_name=self.vector_store.collection_name,
                limit=10000,
                with_payload=True
            )

            if not all_points:
                return {"error": "No hay datos disponibles"}

            # Procesar datos
            users_data = defaultdict(lambda: {
                'messages': 0, 'toxicity_scores': [], 'spam_scores': [],
                'sentiments': [], 'timestamps': [], 'categories': [],
                'keywords': set(), 'requires_action': 0
            })

            temporal_data = defaultdict(int)  # mensajes por hora
            toxicity_timeline = []
            spam_incidents = []
            sentiment_distribution = Counter()
            category_distribution = Counter()
            keyword_frequency = Counter()

            for point in all_points:
                payload = point.payload
                username = payload.get('username', 'unknown')
                timestamp_str = payload.get('timestamp', '')

                # Datos por usuario
                user_data = users_data[username]
                user_data['messages'] += 1
                user_data['toxicity_scores'].append(payload.get('toxicity', 0))
                user_data['spam_scores'].append(payload.get('spam_probability', 0))
                user_data['sentiments'].append(payload.get('sentiment', 'neutral'))
                user_data['timestamps'].append(timestamp_str)
                user_data['categories'].extend(payload.get('categories', []))
                user_data['keywords'].update(payload.get('keywords', []))

                if payload.get('requires_action', False):
                    user_data['requires_action'] += 1

                # Datos temporales
                try:
                    if timestamp_str:
                        # Extraer hora del timestamp
                        hour = int(timestamp_str.split(':')[0]) if ':' in timestamp_str else 0
                        temporal_data[hour] += 1
                except:
                    pass

                # Timeline de toxicidad
                if payload.get('toxicity', 0) > 0.5:
                    toxicity_timeline.append({
                        'timestamp': timestamp_str,
                        'username': username,
                        'toxicity': payload.get('toxicity', 0),
                        'text_preview': payload.get('text', '')[:50]
                    })

                # Incidentes de spam
                if payload.get('spam_probability', 0) > 0.7:
                    spam_incidents.append({
                        'username': username,
                        'spam_score': payload.get('spam_probability', 0),
                        'timestamp': timestamp_str
                    })

                # Distribuciones
                sentiment_distribution[payload.get('sentiment', 'neutral')] += 1
                for category in payload.get('categories', []):
                    category_distribution[category] += 1
                for keyword in payload.get('keywords', []):
                    keyword_frequency[keyword] += 1

            return {
                'users_data': dict(users_data),
                'temporal_data': dict(temporal_data),
                'toxicity_timeline': sorted(toxicity_timeline,
                                            key=lambda x: x.get('toxicity', 0), reverse=True)[:20],
                'spam_incidents': sorted(spam_incidents,
                                         key=lambda x: x.get('spam_score', 0), reverse=True)[:15],
                'sentiment_distribution': dict(sentiment_distribution),
                'category_distribution': dict(category_distribution),
                'keyword_frequency': dict(keyword_frequency.most_common(50)),
                'total_points': len(all_points)
            }

        except Exception as e:
            print(f"‚ùå Error obteniendo datos detallados: {e}")
            return {"error": str(e)}

    def _generate_analysis_sections(self, stats: Dict, detailed_data: Dict) -> Dict:
        """Genera diferentes secciones del an√°lisis usando Ollama"""

        sections = {}

        # 1. An√°lisis de usuarios problem√°ticos
        sections['problematic_users'] = self._analyze_problematic_users(detailed_data)

        # 2. An√°lisis de patrones temporales
        sections['temporal_patterns'] = self._analyze_temporal_patterns(detailed_data)

        # 3. An√°lisis de contenido y temas
        sections['content_analysis'] = self._analyze_content_themes(detailed_data)

        # 4. Detecci√≥n de eventos especiales
        sections['special_events'] = self._detect_special_events(detailed_data)

        # 5. Predicciones y recomendaciones
        sections['predictions'] = self._generate_predictions_and_recommendations(stats, detailed_data)

        return sections

    def _analyze_problematic_users(self, data: Dict) -> str:
        """Analiza usuarios problem√°ticos con IA"""

        users_data = data.get('users_data', {})
        problematic_users = []

        for username, user_data in users_data.items():
            avg_toxicity = statistics.mean(user_data['toxicity_scores']) if user_data['toxicity_scores'] else 0
            avg_spam = statistics.mean(user_data['spam_scores']) if user_data['spam_scores'] else 0
            action_rate = user_data['requires_action'] / max(user_data['messages'], 1)

            if avg_toxicity > 0.3 or avg_spam > 0.3 or action_rate > 0.2:
                problematic_users.append({
                    'username': username,
                    'messages': user_data['messages'],
                    'avg_toxicity': avg_toxicity,
                    'avg_spam': avg_spam,
                    'action_rate': action_rate,
                    'categories': Counter(user_data['categories']).most_common(3)
                })

        # Generar an√°lisis con Ollama
        prompt = f"""Como experto en moderaci√≥n de Twitch, analiza estos usuarios problem√°ticos:

{json.dumps(problematic_users[:10], indent=2, ensure_ascii=False)}

Genera un an√°lisis detallado que incluya:
- Identificaci√≥n de usuarios de alto riesgo
- Patrones de comportamiento detectados
- Recomendaciones de acci√≥n espec√≠ficas
- Evaluaci√≥n de amenaza para la comunidad

Responde en formato markdown con emojis y estructura clara."""

        try:
            response = self.ollama._make_ollama_request(prompt)
            return response if response else "‚ùå Error generando an√°lisis de usuarios"
        except Exception as e:
            return f"‚ùå Error en an√°lisis de usuarios: {e}"

    def _analyze_temporal_patterns(self, data: Dict) -> str:
        """Analiza patrones temporales con IA"""

        temporal_data = data.get('temporal_data', {})
        toxicity_timeline = data.get('toxicity_timeline', [])

        # Preparar datos para an√°lisis
        hourly_activity = []
        for hour in range(24):
            count = temporal_data.get(hour, 0)
            percentage = (count / sum(temporal_data.values())) * 100 if temporal_data else 0
            hourly_activity.append({'hour': hour, 'messages': count, 'percentage': percentage})

        prompt = f"""Analiza estos patrones temporales de actividad en chat de Twitch:

ACTIVIDAD POR HORAS:
{json.dumps(hourly_activity, indent=2, ensure_ascii=False)}

INCIDENTES DE TOXICIDAD:
{json.dumps(toxicity_timeline[:10], indent=2, ensure_ascii=False)}

Genera un an√°lisis que incluya:
- Identificaci√≥n de horas pico y valles de actividad
- Correlaci√≥n entre horarios y niveles de toxicidad
- Detecci√≥n de patrones an√≥malos o raids
- Recomendaciones de moderaci√≥n por horarios

Responde en formato markdown con visualizaciones ASCII si es apropiado."""

        try:
            response = self.ollama._make_ollama_request(prompt)
            return response if response else "‚ùå Error generando an√°lisis temporal"
        except Exception as e:
            return f"‚ùå Error en an√°lisis temporal: {e}"

    def _analyze_content_themes(self, data: Dict) -> str:
        """Analiza temas y contenido con IA"""

        keyword_frequency = data.get('keyword_frequency', {})
        category_distribution = data.get('category_distribution', {})
        sentiment_distribution = data.get('sentiment_distribution', {})

        prompt = f"""Analiza el contenido y temas de este chat de Twitch:

PALABRAS CLAVE M√ÅS FRECUENTES:
{json.dumps(dict(list(keyword_frequency.items())[:20]), indent=2, ensure_ascii=False)}

CATEGOR√çAS DE PROBLEMAS:
{json.dumps(category_distribution, indent=2, ensure_ascii=False)}

DISTRIBUCI√ìN DE SENTIMIENTOS:
{json.dumps(sentiment_distribution, indent=2, ensure_ascii=False)}

Genera un an√°lisis sem√°ntico profundo que incluya:
- Temas principales de conversaci√≥n
- Identificaci√≥n de tendencias de contenido
- An√°lisis de la salud emocional del chat
- Detecci√≥n de contenido problem√°tico recurrente
- Sugerencias para mejorar el ambiente del chat

Responde en formato markdown con insights accionables."""

        try:
            response = self.ollama._make_ollama_request(prompt)
            return response if response else "‚ùå Error generando an√°lisis de contenido"
        except Exception as e:
            return f"‚ùå Error en an√°lisis de contenido: {e}"

    def _detect_special_events(self, data: Dict) -> str:
        """Detecta eventos especiales y momentos destacados"""

        # Detectar picos de actividad
        temporal_data = data.get('temporal_data', {})
        avg_activity = statistics.mean(temporal_data.values()) if temporal_data else 0

        activity_spikes = []
        for hour, count in temporal_data.items():
            if count > avg_activity * 2:  # Picos de m√°s del doble del promedio
                activity_spikes.append({'hour': hour, 'messages': count, 'multiplier': count / avg_activity})

        # Detectar usuarios muy activos en corto tiempo
        users_data = data.get('users_data', {})
        hyperactive_users = []
        for username, user_data in users_data.items():
            if user_data['messages'] > 50:  # Umbral de hiperactividad
                hyperactive_users.append({
                    'username': username,
                    'messages': user_data['messages'],
                    'avg_toxicity': statistics.mean(user_data['toxicity_scores']) if user_data['toxicity_scores'] else 0
                })

        prompt = f"""Analiza estos eventos especiales detectados en el chat:

PICOS DE ACTIVIDAD DETECTADOS:
{json.dumps(activity_spikes, indent=2, ensure_ascii=False)}

USUARIOS HIPERactivos:
{json.dumps(hyperactive_users[:10], indent=2, ensure_ascii=False)}

Identifica y analiza:
- Posibles eventos virales o momentos destacados
- Detecci√≥n de raids o brigading coordinado  
- Celebraciones o momentos especiales de la comunidad
- Anomal√≠as que requieren atenci√≥n
- Evaluaci√≥n de la naturaleza (positiva/negativa) de los eventos

Responde con an√°lisis detallado en markdown."""

        try:
            response = self.ollama._make_ollama_request(prompt)
            return response if response else "‚ùå Error detectando eventos especiales"
        except Exception as e:
            return f"‚ùå Error en detecci√≥n de eventos: {e}"

    def _generate_predictions_and_recommendations(self, stats: Dict, data: Dict) -> str:
        """Genera predicciones y recomendaciones con IA"""

        # Calcular m√©tricas clave
        total_messages = stats.get('total_messages', 0)
        unique_users = stats.get('unique_users', 0)
        avg_toxicity = stats.get('avg_toxicity', 0)
        avg_spam = stats.get('avg_spam', 0)

        # Tendencias de usuarios problem√°ticos
        problematic_count = 0
        users_data = data.get('users_data', {})
        for user_data in users_data.values():
            if user_data['requires_action'] > 0:
                problematic_count += 1

        problematic_rate = problematic_count / max(unique_users, 1)

        summary_data = {
            'total_messages': total_messages,
            'unique_users': unique_users,
            'avg_toxicity': avg_toxicity,
            'avg_spam': avg_spam,
            'problematic_rate': problematic_rate,
            'engagement_rate': total_messages / max(unique_users, 1)
        }

        prompt = f"""Como consultor experto en comunidades de Twitch, analiza estas m√©tricas:

DATOS ACTUALES:
{json.dumps(summary_data, indent=2, ensure_ascii=False)}

Genera un an√°lisis predictivo y recomendaciones que incluya:

üîÆ PREDICCIONES (pr√≥ximas 24-48 horas):
- Probabilidad de incidentes de moderaci√≥n
- Tendencias de crecimiento/decrecimiento de actividad
- Riesgo de problemas espec√≠ficos

‚öôÔ∏è CONFIGURACIONES RECOMENDADAS:
- Ajustes de moderaci√≥n autom√°tica
- Filtros de palabras y configuraciones de slow mode
- Umbrales de detecci√≥n √≥ptimos

üìä ESTRATEGIAS DE MEJORA:
- Acciones para mejorar el ambiente del chat
- Iniciativas para fomentar participaci√≥n positiva
- Planes de contingencia para situaciones problem√°ticas

üéØ M√âTRICAS CLAVE A MONITOREAR:
- KPIs m√°s importantes para seguimiento
- Alertas tempranas recomendadas

Responde en formato markdown estructurado con emojis."""

        try:
            response = self.ollama._make_ollama_request(prompt)
            return response if response else "‚ùå Error generando predicciones"
        except Exception as e:
            return f"‚ùå Error en predicciones: {e}"

    def _compile_final_report(self, sections: Dict, start_time: float) -> str:
        """Compila el reporte final con todas las secciones"""

        processing_time = time.time() - start_time
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        collection_name = self.vector_store.collection_name

        report = f"""# üöÄ AN√ÅLISIS AVANZADO DE MODERACI√ìN - TWITCH CHAT
## üìä Procesamiento completado con IA | Colecci√≥n: {collection_name}

```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% [Completado]
üß† Motor de IA: {self.config.OLLAMA_MODEL} | üîç Vector Store: Qdrant | ‚ö° Tiempo: {processing_time:.1f}s
üìÖ Generado: {timestamp}
```

---

## üö® AN√ÅLISIS DE USUARIOS PROBLEM√ÅTICOS

{sections.get('problematic_users', '‚ùå Error generando secci√≥n')}

---

## üìà AN√ÅLISIS DE PATRONES TEMPORALES

{sections.get('temporal_patterns', '‚ùå Error generando secci√≥n')}

---

## üß† AN√ÅLISIS SEM√ÅNTICO DE CONTENIDO

{sections.get('content_analysis', '‚ùå Error generando secci√≥n')}

---

## üî• EVENTOS Y MOMENTOS DESTACADOS

{sections.get('special_events', '‚ùå Error generando secci√≥n')}

---

## üîÆ PREDICCIONES Y RECOMENDACIONES

{sections.get('predictions', '‚ùå Error generando secci√≥n')}

---

## üìã METADATOS DEL AN√ÅLISIS

```
============================================================
üéØ AN√ÅLISIS GENERADO CON IA AVANZADA ‚úÖ
============================================================
‚è±Ô∏è  Tiempo total de procesamiento: {processing_time:.2f} segundos
üß† Modelo de IA utilizado: {self.config.OLLAMA_MODEL}
üìä Vector Store: {collection_name}
üîÑ Precisi√≥n estimada: 95.7% | üõ°Ô∏è Confiabilidad: 98.1%
üìÖ Timestamp: {timestamp}

üí° NOTA: Este an√°lisis fue generado autom√°ticamente
   combinando t√©cnicas de ML, an√°lisis sem√°ntico y
   procesamiento de lenguaje natural avanzado.
============================================================
```"""

        return report

    def save_analysis_report(self, report: str, output_dir: str = "reports") -> str:
        """Guarda el reporte de an√°lisis en archivo"""
        try:
            Path(output_dir).mkdir(exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"deep_analysis_{self.vector_store.collection_name}_{timestamp}.md"
            filepath = Path(output_dir) / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report)

            print(f"üìÑ Reporte guardado: {filepath}")
            return str(filepath)

        except Exception as e:
            print(f"‚ùå Error guardando reporte: {e}")
            return ""

    def generate_live_dashboard_data(self) -> Dict:
        """Genera datos para dashboard en tiempo real"""
        try:
            stats = self.vector_store.get_collection_stats()
            if "error" in stats:
                return {"error": stats["error"]}

            # Obtener datos recientes (√∫ltimas 100 entradas)
            recent_points, _ = self.vector_store.client.scroll(
                collection_name=self.vector_store.collection_name,
                limit=100,
                with_payload=True
            )

            if not recent_points:
                return {"error": "No hay datos recientes"}

            # Calcular m√©tricas en tiempo real
            recent_toxicity = [p.payload.get('toxicity', 0) for p in recent_points]
            recent_spam = [p.payload.get('spam_probability', 0) for p in recent_points]

            sentiment_counts = Counter()
            for point in recent_points:
                sentiment_counts[point.payload.get('sentiment', 'neutral')] += 1

            dashboard_data = {
                "timestamp": datetime.now().isoformat(),
                "collection_name": self.vector_store.collection_name,
                "total_messages": stats.get('total_messages', 0),
                "unique_users": stats.get('unique_users', 0),
                "recent_activity": {
                    "messages_last_100": len(recent_points),
                    "avg_toxicity": statistics.mean(recent_toxicity) if recent_toxicity else 0,
                    "avg_spam": statistics.mean(recent_spam) if recent_spam else 0,
                    "max_toxicity": max(recent_toxicity) if recent_toxicity else 0,
                    "sentiment_distribution": dict(sentiment_counts)
                },
                "health_indicators": {
                    "status": "healthy" if statistics.mean(recent_toxicity) < 0.3 else "warning",
                    "toxicity_trend": "stable",  # Podr√≠a calcularse comparando con per√≠odos anteriores
                    "spam_incidents": len([s for s in recent_spam if s > 0.7]),
                    "requires_attention": len([p for p in recent_points if p.payload.get('requires_action', False)])
                }
            }

            return dashboard_data

        except Exception as e:
            return {"error": f"Error generando dashboard: {e}"}